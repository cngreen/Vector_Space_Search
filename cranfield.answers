DESCRIPTION OF WEIGHTING SCHEMES USED:-------------------------------------------
Document:
tf-idf: 
	tf = raw term frequency, the number of times the term is used in the document
	idf = log(N/n) where N is the total number of documents and n is the documents containing the term
kari:
	tf = normalized term frequency, n-tf = 0.5 + 0.5 * (tf/max{tf})
	idf = log(N/n)
	normalized by Euclidian vector length
	rationale: normalizing the term frequency gives a less dramatic weighting to documents that have repeated terms; normalizing by vector length accounts for larger documents having a higher probability of containing a relevant term
Query:
tf-idf:
	tf = raw term frequency, the number of times the term is used in the query
	idf = log(N/n) where N is the total number of documents and n is the documents containing the term
kari:
	tf = raw term frequency
	prob-idf = log(N-n/n)
	rationale: rare repeated words in queries; probabilistic inverse collection frequency gives more weight to how rare a search term is

# -------------------------------------------------------------------------------
MACRO-AVERAGED PRECISION AND RECALL:

top-10:
	tfidf-tfidf: 
		precision: 0.0511
		recall: 0.0790
	kari-kari:
		precision: 0.1773
		recall: 0.2821

top-50:
	tfidf-tfidf: 
		precision: 0.0263
		recall: 0.1865
	kari-kari:
		precision: 0.0708
		recall: 0.4860

top-100:
	tfidf-tfidf: 
		precision: 0.0187
		recall: 0.2515
	kari-kari:
		precision: 0.0443
		recall: 0.5919

top-500:
	tfidf-tfidf: 
		precision: 0.0097
		recall: 0.5992
	kari-kari:
		precision: 0.0129
		recall: 0.7801

# -------------------------------------------------------------------------------
WHICH WEIGHTING SCHEME PROVIDES BETTER RESULTS:

